---
title: "example_prep_non_spatial.rmd"
output: html_document
---


_Why prep in R and/or Rmarkdown? If you havne't used Github/Rstudio combination for work before, this process might seem daunting at first. But this is a hihgly effective system for teams to work collaboratively, record decisions, and share results easily within or outside your team. You might have relied on emails for communciation and passing versions of datasets back and forth only to be lost and confused in the process... 

If someone wants to see where your data comes from, how you have processed the data, and what's the rationale for including or excluding certain data, they can come to this file and at a glance know your data preparation process. Also, it will be helpful to yourself a few months, or years from now to see what you had done. If there are personale changes in your team during the project, it would be easy for the transition if your data prepation has been kept in one place and documented clearly. You can do this with comments in R. However, we would recommend Rmarkdown language, so that you can render your comments, r scripts, and plots_

_Data prep often takes the biggest chunk of your time during an assessment, more than the final calculation of the OHI scores itself! The first repository you will receive aims to help you wade through this mundane yet important first step of the assessment. Treat this prep document as your notebook, calculator, and presentation of your work. 

Motivation: transparency, recording, sharing, defending your work; for your future self, team members, and anyone who's interested in how you have prepared your data. 

OHI is open science and we always promote transparency in every step of the assessment. Recording decision makings, such as why you have included or excluded certain data, in your assessment is one of the best practices we highly recomend to anyone who is interested in OHI, or open science in general. Instead of one individual member writing down information in her own notebook or computer, Github allows shared team memory and collaboration. Any team member can review and track data preparation process. Your future self can see what you have done a year ago. 

The purpose of this script is to explore raw data and see whether it makes sense to use them in your final calculations. If it does, you can format it further and save them as data layers. 

Rmarkdown produces a report with R code and figures/tables embedded in it. It can be rendered as HTML to communicate with other people. However, if you prefer to not deal with a new language, you can also write r scripts, with comments tucked into it. See ( ) for example in r.

In this example data preparation document, we'll walk through the general steps we recommend you include in your document, including:

- Process (use two BHI examples: one non-spatial and one spatial) 
  - general introduction of goal, goal model, and data 
  - load data
  - change to long format and save as .csv 
  - explore raw data and see whether it makes sense
  - plot and describe 
  - explore status and trend calcluations (then move to functions.r)
  - save in layers folder and register data layers in layers.csv
  
  
_In the Baltic example, there are two sources of data for secchi depth measurements. At this moment, we don't know which set is more suitable, or whether or how to combine the two sources. You'll see the process of exploring the two data sets and record the observations and subsequent decisions made on how to deal with the two data sets... including commonly used R commands to manipulate and plot data..._

NOTE TO SELF: 
- status and trend calculations here? note as optional? 


# Intro: 

This section you can introduce the goal, what types of information or data are needed, data sources, the goal model, and how to approach trend calculation. We'll use Clean Water goal (modified from [OHI-Baltic](https://github.com/OHI-Science/bhi/blob/draft/baltic2015/prep/CW/secchi/secchi_prep.Rmd)) as an example: 

modified for illustration ... ... 

# Background

_Here you can give a general introduction of what this goal/subgoal is trying to measure, what it means in your local context, and what parameters make sense to be included or explored here, etc._ 

_One component of CW goal: nutrients data, as represented by secchi depth measured in summer time._

Summer time water clarity, measured by secchi depth, is used as a proxy indicator for nutrients in the water. More info can be found [here](http://www.helcom.fi/baltic-sea-trends/indicators/water-clarity). 

# Goal model

_Record what the goal model and reference point should be, how to approach trend calculations, etc._ 

## Status

Xao = Mean Stock Indicator Value / Reference pt
Stock indicators = two HELCOM core indicators assessed for good environemental status (each scored between 0 and 1 by BHI)
Reference pt = maximum possible good environmental status (value=1)

## Trend

CPUE time series are available for all stations used for the HELCOM coastal fish populations core indicators. These data were provided by Jens Olsson (FISH PRO II project). To calculate GES status, full time series were used. Therefore, only one status time point and cannot calculate trend of status over time. Instead, follow approach from Bergström et al 2016, but only focus on the final time period for the slope (2004-2013).

Bergstrom et al. 2016. Long term changes in the status of coastal fish in the Baltic Sea. Estuarin, Coast and Shelf Science. 169:74-84

Method: 

1. Select final time period of trend assessment (2004-2013)
2. Use time series from both indicators, Key Species and Functional groups. For functional groups,include both cyprinid and piscivore time series.
3. For each time series: square-root transform data, z-score, fit linear regression, extract slope
4. Within each time series group (key species, cyprinid, piscivore), take the mean slope for each group within each basin
5. Within each basin take a mean functional group indicator slope (mean of cyprinid mean and piscivore mean)
6. For each basin take overall mean slope - mean of key species and functional group
7. Apply trend value for basin to all BHI regions (except in Gulf of Finland, do not apply Finnish site value to Estonia and Russian regions.)


## Data sources

_Where the data comes from, where it's stored, potential concerns with the data, why you included or excluded certain data, etc:_


ICES
Data extracted from database and sent by Hjalte Parner on Feb 10 2016.

Note from Parner: "extraction from our database classified into HELCOM Assessment Units – HELCOM sub basins with coastal WFD water bodies or water types"

SMHI
Downloaded from SMHI [Shark database](http://www.smhi.se/klimatdata/oceanografi/havsmiljodata/marina-miljoovervakningsdata) on Feb 23 2016 by Lena Viktorsson.

Download notes: datatyp: Physical and Chemical; Parameter: secchi depth
Lena did not exclude any data when she downloaded it.

_Other comments you could also record here:_

Pros and cons of using these data: 
- Pros: most recent published data and thus reflect the most current conditions of ... 
- Cons: these datasets don't have full spatial coverage, or don't have continuous temporal coverage...

Reasons for exlcuding certain datasets: 

Direct measurements of nutrient levels (eg. phosphate, nitrate, etc) were excluded from this subgoal because not every region measure these chemicals regularly; _OR_ we didn't have time-series data on nutrients to be able to 


# Data prep process

## setup 

This section will set up directories, functions, call commonly used libraries, etc, to prepare for the next steps of data prep. 

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

## load common libraries, directories, functions, etc

## Libraries
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(RMySQL)
library(stringr)
library(tools)
library(stringr)
library(rprojroot)

## Directories
dir_baltic = '~/github/ohimanual/tutorials'  # '~/github/bhi/baltic2015' 
dir_layers = file.path(dir_baltic, 'layers')
dir_prep   = file.path(dir_baltic, 'data_prep_tutorial') # file.path(dir_baltic, 'prep')
dir_ao    = file.path(dir_prep, 'AO')

```

#### Read in data and initial exploration

```{r read in data, echo = FALSE}

# read in first data set
data1 = read_csv(file.path(dir_prep, 'example_data', 'ices_secchi.csv'))

# quick review of dataset 
head(data1)
dim(data1)

# read in second data set
data2 = read_csv(file.path(dir_prep, 'example_data', 'smhi_secchi.csv'))

# quick review of data
head(data2)
dim(data2)

## NOTE: examples of other functions you could also use for data review
# colnames(data1)
# str(data1)

# initial filtering

ices <- data1 %>% data.frame()%>%
  dplyr::select(bhi_id= BHI_ID, secchi, year= Year, month= Month, 
         lat= Latitude, lon = Longitude, 
         cruise= Cruise, station = Station, date= Date) %>%
  mutate(date = as.Date(date, format= "%Y-%m-%d"))%>%
  mutate(supplier = 'ices')

head(ices)

## which ices data have BHI_ID of NA
ices.na <- ices %>%
           filter(is.na(bhi_id))

nrow(ices.na) # counted total of 1684  

## potential further exploration of those locations: 
# ices.na.loc = ices.na %>% dplyr::select(lat,lon) %>% distinct() ## unique locations; using dplyr::select here because select is also a function in a different r package. specifying it's from the dplyr package (dply::) avoids confusion and downstream error. 
# 
# nrow(ices.na.loc) # 86  

# # remove data with no BHI_ID 
# 
# ices <- ices %>% 
#   filter(!is.na(bhi_id))



smhi <- data2 %>% data.frame()%>%
  rename(secchi = value) %>%
  dplyr::select(bhi_id= BHI_ID, secchi, year= Year, month= Month, 
        lat= Latitude, lon= Longitude, 
         cruise = Provtagningstillfaelle.id, 
         station = Stationsnamn, date= Date, coast_code=HELCOM_COASTAL_CODE) %>%
  mutate(supplier = 'smhi', cruise = as.character(cruise))
head(smhi)


## Look for duplicate data

## is any data duplicated in ices itself
ices.duplicated = duplicated(ices)
sum(ices.duplicated==TRUE) #181855  ## MANY duplicates 

# is it because multiple cruises go out on the same day and same location?
ices.duplicated = duplicated(dplyr::select(ices,-station)) # remove station column
sum(ices.duplicated==TRUE) #181977 ## more duplicated
# so this is not the reason

# remove lat, lon
ices.duplicated.latlon = duplicated(dplyr::select(ices,-lat, -lon)) %>% sum()
sum(ices.duplicated.latlon)


## is any data duplicated in smhi itself
smhi.duplicated = duplicated(dplyr::select(smhi, -station))
sum(smhi.duplicated==TRUE) #85691 ## MANY duplicates  ## removing station does not affect it
new_smhi = unique(dplyr::select(smhi, -station)); nrow(new_smhi) #take only unique records # 17099

## use setdiff() to indentify data smhi not in ices
new_smhi = setdiff(dplyr::select(new_smhi,-supplier,-cruise), dplyr::select(new_ices,-supplier,-cruise)) %>%
            mutate(supplier = "smhi")
nrow(new_smhi) #  16627
## it appears 461 records are duplicates (if remove cruise and station)
## if date, lat, lon, secchi all match, I think they are duplicates

## Now create a new allData, bind only the new_smhi object to ices
allData = bind_rows(new_ices,new_smhi)
nrow(allData) # 50193
allData %>% dplyr::select(year, month, date, cruise, lat, lon,secchi) %>% distinct() %>%nrow(.)  #50193

## what if remove cruise
allData %>% dplyr::select(year, month, date, lat, lon,secchi) %>% distinct() %>%nrow(.)
# 50193


```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
