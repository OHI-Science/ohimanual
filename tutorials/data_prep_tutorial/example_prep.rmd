---
title: "example_prep_non_spatial.rmd"
output: html_document
---


Data prep often takes the biggest chunk of your time during an assessment, more than the final calculation of the OHI scores itself! The first repository you will receive aims to help you wade through this mundane yet important first step of the assessment. Treat this prep document as your notebook, calculator, and presentation of your work. We have always tried to take full advantage of the open science tools, such as Rmarkdown and Github, which we found to be the best combination for record keeping and communication. 

Why prep in R and/or Rmarkdown? If you havne't used Github/Rstudio combination for work before, this process might seem daunting at first. But this is a hihgly effective system for teams to work collaboratively, record decisions, and share results easily within or outside your team. You might have relied on emails for communciation and passing versions of datasets back and forth only to be lost and confused in the process... 

If someone wants to see where your data comes from, how you have processed the data, and what's the rationale for including or excluding certain data, they can come to this file and at a glance know your data preparation process. Also, it will be helpful to yourself a few months, or years from now to see what you had done. If there are personale changes in your team during the project, it would be easy for the transition if your data prepation has been kept in one place and documented clearly. You can do this with comments in R. However, we would recommend Rmarkdown language, so that you can render your comments, r scripts, and plots_

Motivation: transparency, recording, sharing, defending your work; for your future self, team members, and anyone who's interested in how you have prepared your data. 

OHI is open science and we always promote transparency in every step of the assessment. Recording decision makings, such as why you have included or excluded certain data, in your assessment is one of the best practices we highly recomend to anyone who is interested in OHI, or open science in general. Instead of one individual member writing down information in her own notebook or computer, Github allows shared team memory and collaboration. Any team member can review and track data preparation process. Your future self can see what you have done a year ago. 

The purpose of this script is to explore raw data and see whether it makes sense to use them in your final calculations. If it does, you can format it further and save them as data layers. 

Rmarkdown produces a report with R code and figures/tables embedded in it. It can be rendered as HTML to communicate with other people. However, if you prefer to not deal with a new language, you can also write r scripts, with comments tucked into it. See ( ) for example in r.

In this example data preparation document, we'll walk through the general steps we recommend you include in your document, including:

- Process (use two BHI examples: one non-spatial and one spatial) 
  - general introduction of goal, goal model, and data 
  - load data
  - change to long format and save as .csv 
  - explore raw data and see whether it makes sense
  - plot and describe 
  - explore status and trend calcluations (then move to functions.r)
  - save in layers folder and register data layers in layers.csv
  
  
_In the Baltic example, there are two sources of data for secchi depth measurements. At this moment, we don't know which set is more suitable, or whether or how to combine the two sources. You'll see the process of exploring the two data sets and record the observations and subsequent decisions made on how to deal with the two data sets... including commonly used R commands to manipulate and plot data..._

NOTE TO SELF: 
- status and trend calculations here? note as optional? 


# Intro: 

This section you can introduce the goal, what types of information or data are needed, data sources, the goal model, and how to approach trend calculation. We'll use Clean Water goal (modified from [OHI-Baltic](https://github.com/OHI-Science/bhi/blob/draft/baltic2015/prep/CW/secchi/secchi_prep.Rmd)) as an example: 

modified for illustration ... ... 

# Background

_Here you can give a general introduction of what this goal/subgoal is trying to measure, what it means in your local context, and what parameters make sense to be included or explored here, etc._ 

_One component of CW goal: nutrients data, as represented by secchi depth measured in summer time._

Summer time water clarity, measured by secchi depth, is used as a proxy indicator for nutrients in the water. More info can be found [here](http://www.helcom.fi/baltic-sea-trends/indicators/water-clarity). 

# Goal model

_Record what the goal model and reference point should be, how to approach trend calculations, etc._ 

## Status

Xao = Mean Stock Indicator Value / Reference pt
Stock indicators = two HELCOM core indicators assessed for good environemental status (each scored between 0 and 1 by BHI)
Reference pt = maximum possible good environmental status (value=1)

## Trend

CPUE time series are available for all stations used for the HELCOM coastal fish populations core indicators. These data were provided by Jens Olsson (FISH PRO II project). To calculate GES status, full time series were used. Therefore, only one status time point and cannot calculate trend of status over time. Instead, follow approach from Bergström et al 2016, but only focus on the final time period for the slope (2004-2013).

Bergstrom et al. 2016. Long term changes in the status of coastal fish in the Baltic Sea. Estuarin, Coast and Shelf Science. 169:74-84

Method: 

1.select final time period of trend assessment (2004-2013)
2. Use time series from both indicators, Key Species and Functional groups. For functional groups,include both cyprinid and piscivore time series.
3. For each time series: square-root transform data, z-score, fit linear regression, extract slope
4. Within each time series group (key species, cyprinid, piscivore), take the mean slope for each group within each basin
5. Within each basin take a mean functional group indicator slope (mean of cyprinid mean and piscivore mean)
6. For each basin take overall mean slope - mean of key species and functional group
7. Apply trend value for basin to all BHI regions (except in Gulf of Finland, do not apply Finnish site value to Estonia and Russian regions.)


## Data sources

_Where the data comes from, where it's stored, potential concerns with the data, why you included or excluded certain data, etc:_


ICES
Data extracted from database and sent by Hjalte Parner on Feb 10 2016.

Note from Parner: "extraction from our database classified into HELCOM Assessment Units – HELCOM sub basins with coastal WFD water bodies or water types"

SMHI
Downloaded from SMHI [Shark database](http://www.smhi.se/klimatdata/oceanografi/havsmiljodata/marina-miljoovervakningsdata) on Feb 23 2016 by Lena Viktorsson.

Download notes: datatyp: Physical and Chemical; Parameter: secchi depth
Lena did not exclude any data when she downloaded it.

_Other comments you could also record here:_

Pros and cons of using these data: 
- Pros: most recent published data and thus reflect the most current conditions of ... 
- Cons: these datasets don't have full spatial coverage, or don't have continuous temporal coverage...

Reasons for exlcuding certain datasets: 

Direct measurements of nutrient levels (eg. phosphate, nitrate, etc) were excluded from this subgoal because not every region measure these chemicals regularly; _OR_ we didn't have time-series data on nutrients to be able to 


# Data prep process

## setup 

This section will set up directories, functions, call commonly used libraries, etc, to prepare for the next steps of data prep. 

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

## load common libraries, directories, functions, etc

## Libraries
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(tools)
library(stringr)

## Directories
dir_baltic = '~/github/ohimanual/tutorials'  # '~/github/bhi/baltic2015' 
dir_layers = file.path(dir_baltic, 'layers')
dir_prep   = file.path(dir_baltic, 'data_prep_tutorial') # file.path(dir_baltic, 'prep')
dir_ao    = file.path(dir_prep, 'AO')

```

#### Read in data and initial exploration

ICES and SMHI data have non-overlapping observations and were combined to one data set for our use. Some observations were not assigned to any regions (ie. no region IDs attached) and were omitted. 

Both data sets contains profile data (eg temperature, but secchi is only measured once). We need only unique secchi records. Duplicates were thus taken out. 

```{r read in data, echo = FALSE}

#### Read in raw data files 

## read in ices set
data1 = read_csv(file.path(dir_prep, 'example_data', 'ices_secchi.csv'))

## quick review of dataset 
head(data1)
str(data1)

## read in smhi set
data2 = read_csv(file.path(dir_prep, 'example_data', 'smhi_secchi.csv'))

head(data2)
str(data2)

##### NOTE: examples of other functions you could also use for data review
## colnames(data1)
## dim(data1) 

#### Initial filtering

ices <- data1 %>% data.frame()%>%
  select(bhi_id= BHI_ID, secchi, year= Year, month= Month, 
         lat= Latitude, lon = Longitude, date= Date) %>%
  mutate(date = as.Date(date, format= "%Y-%m-%d"))%>%
  mutate(supplier = 'ices')

head(ices)
str(ices)

#### NOTE: " " identifies the package where the function "select" comes from. It's used here becauseselect is also a function from a different package that's loaded  

## which ices data have BHI_ID of NA
ices.na <- ices %>%
           filter(is.na(bhi_id)) 

nrow(ices.na) # counted total of 1684  

## you could do further explorations on why these observations don't have an ID attached. but for simplicity and illustration, 
## we will ignore bhi_id = NA 
ices <- ices %>% 
  filter(!is.na(bhi_id))

###### NOTE: there are other options for data explorations too that are omitted here. For more details, check out section 3.1 of:
###### https://github.com/OHI-Science/bhi/blob/draft/baltic2015/prep/CW/secchi/secchi_prep.Rmd

smhi <- data2 %>% data.frame()%>%
  rename(secchi = value) %>%
  select(bhi_id= BHI_ID, secchi, year= Year, month= Month, 
        lat= Latitude, lon= Longitude, date= Date) %>%
  mutate(supplier = 'smhi') %>% 
  filter(!is.na(bhi_id))
head(smhi)

#### Remove duplicated data within each data set

## is any data duplicated in ices itself
ices.duplicated = duplicated(ices)
sum(ices.duplicated==TRUE) #180963  ## MANY duplicates 

## keep only unique records
new_ices = unique(ices); nrow(new_ices)  #take only unique records # 33018

## is any data duplicated in smhi itself; keeping only unique records
smhi.duplicated = duplicated(select(smhi, -station))
sum(smhi.duplicated==TRUE) #56966 ## MANY duplicates  ## removing station does not affect it
new_smhi = unique(smhi); nrow(new_smhi) #take only unique records # 17090

#### Combine two data sets

## use setdiff() to indentify data smhi not in ices
new_smhi = setdiff(select(new_smhi,-supplier), select(new_ices,-supplier)) %>%
            mutate(supplier = "smhi")
nrow(new_smhi) #10357
## it appears 461 records are duplicates (if remove cruise and station)
## if date, lat, lon, secchi all match, I think they are duplicates

## Now create a new allData, bind only the new_smhi object to ices
allData = bind_rows(new_ices, new_smhi)
nrow(allData) #43253

## double check that there are no duplicates after combining two data sets
allData %>% select(year, month, date, lat, lon,secchi) %>% distinct() %>% nrow(.) #43253

```

####select only summer obervations 

Only summer months post year 2000 were relevant to our use. Therefore we filtered for data in: 

- Months 6-9 (June, July, August, September)  
- Years 2010-2015

The plots showed that some regions don't have good data coverages. Some basins are missing data for most recent years, such as regions 22 and 25. It appeared that water quality data makes more sense at the basin level, and will be aggregated  to the basin level in the next section. (This is an uncommon case in the Baltic's case, but it's left here for demonstration. )

``` {rselect summer data}

summer = allData %>% filter(month %in%c(6:9)) %>%
        filter(year %in% c(2000:2015))
head(summer)

## plot: by month

ggplot(summer) + geom_point(aes(month, secchi, colour=supplier))+
  facet_wrap(~bhi_id, scales ="free_y")

## plot: by year

ggplot(summer) + geom_point(aes(year,secchi, colour=supplier))+
  facet_wrap(~bhi_id)

```

#### combine data to basin level

```{r combine data to basin level}

# read in data to match basin and region  

basin_lookup = read.csv(file.path(dir_prep, 'example_data', 'bhi_basin_country_lookup.csv'), sep=";") %>%
  select(bhi_id = BHI_ID, basin_name=Subbasin)


# add basin name to regions (bhi_id)

summer = summer %>% full_join(., basin_lookup, by="bhi_id")

#Plot by month and by year

ggplot(summer) + geom_point(aes(month,secchi, colour=supplier))+
  facet_wrap(~basin_name)

ggplot(summer) + geom_point(aes(year,secchi, colour=supplier))+
  facet_wrap(~basin_name)

```

#### Calculate mean monthly secchi depth by basin

``` {r mean monthly secchi depth}

## calculate monthly means for each month
mean_months = summer %>% select(year, month,basin_name, secchi) %>%
              group_by(year, month, basin_name) %>%
              summarise(mean_secchi = round(mean(secchi, na.rm=TRUE), 1)) %>%
              ungroup()
head(mean_months)

## plot monthly means 
ggplot(mean_months) + geom_point(aes(year,mean_secchi, colour=factor(month))) +
  geom_line(aes(year, mean_secchi, colour=factor(month))) +
  facet_wrap(~basin_name)+
  scale_y_continuous(limits = c(0,10))

## calculate summer means by basin
## basin summer means = mean of basin monthly mean values

mean_months_summer = mean_months %>% select(year, basin_name, mean_secchi) %>%
                      group_by(year, basin_name)%>%
                      summarise(mean_secchi = round(mean(mean_secchi, na.rm=TRUE), 1)) %>%
                      ungroup()  #in mean calculation all some months to have NA, ignore for that years calculation

## plot summer means by basin
ggplot(mean_months_summer) + geom_point(aes(year,mean_secchi))+
  geom_line(aes(year,mean_secchi))+
  facet_wrap(~basin_name)+
  scale_y_continuous(limits = c(0,10))

```

#### Deaggregate data to region level  

Data needs to be deaggregated to the regional level again for status and trend calculations. 


```{r deaggregate to region level}

mean_summer_rgn = mean_months_summer %>% full_join(basin_lookup, by='basin_name') %>% 
 select(rgn_id = bhi_id, year, mean_secchi)

## save data layer in layer folder
write_csv(mean_summer_rgn, file.path(dir_layers, 'CW_secchi_bhi2015_NJ.csv')) # 'Goal_layer_name_AssessmentCode_YourInitial.csv'

```

#### _Next steps_

Now you have prepared the data layer(s), you can register it in `layers.csv`. _You'll notice that in the original Baltic prep document status and trend were also explored there, instead of functions.r. You could choose to do the same, especially if you want to explore different equations or approaches to trend calculations._







